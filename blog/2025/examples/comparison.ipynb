{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Like any good analysis I am doing this in a [Jupyter Notebook](https://jupyter.org/). First we load the CSV data:",
   "id": "d952ea341f78d94a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas\n",
    "\n",
    "baseline = pandas.read_csv(\"baseline.csv\")[\"response_time\"]\n",
    "task_handler = pandas.read_csv(\"task_handler.csv\")[\"response_time\"]"
   ],
   "id": "bd5ba89ee0d90c1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then plot the distribution of response times, to see how they compare:",
   "id": "cd46ca673428de3"
  },
  {
   "cell_type": "code",
   "id": "0bb368c8-a09f-41ad-8afb-c5d065b504b0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.hist(baseline, density=True, bins=1000, label=\"Baseline\", alpha=0.5)\n",
    "pyplot.hist(task_handler, density=True, bins=1000, label=\"Task Handler\", alpha=0.5)\n",
    "pyplot.ylabel(\"Request Density\")\n",
    "pyplot.xlabel(\"Response Time\")\n",
    "pyplot.legend(loc=\"upper right\")\n",
    "pyplot.xlim(0, 25)\n",
    "\n",
    "pyplot.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From the plotted data we can see that the response times look to have improved. With the median response time being almost 30% less time:",
   "id": "513cc9b46ca410f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(task_handler.median() - baseline.median()) / baseline.median()",
   "id": "9dbd236ef782cd68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we need to prove that the change was not random chance. For that we are going to use statistics, in this case the unpaired t-test, which, like all good statistics, comes from an interesting source: Guinness! We will be using the [Welch's t-test](https://en.wikipedia.org/wiki/Welch's_t-test) variant as it accounts for a different number of samples, which we will have.\n",
    "\n",
    "The statistic we care about is called the p-value, which we calculate as such:"
   ],
   "id": "7f121374921c5fbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.ttest_ind(baseline, task_handler, equal_var=False).pvalue"
   ],
   "id": "e17ad4b99418faf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now what does this mean? A p-value is a number between `0`, and `1` that helps us decide whether the results are likely due to chance, or if they suggest a real effect.\n",
    "\n",
    " Think of it this way: if we assume there is no real difference, or effect (the null hypothesis), the p-value tells us how likely it is that we would see the results we observed by just random chance.\n",
    "\n",
    "A p-value below `0.05` is often considered \"statistically significant\", meaning there's a less than 5% chance the results occurred due to random variation.\n",
    "\n",
    "In this case the value is 0 (probably due to rounding errors) meaning we are definitely below `0.05` meaning we have very likely improved performance."
   ],
   "id": "31474d88dce09a3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
